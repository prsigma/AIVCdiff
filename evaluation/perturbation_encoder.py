from torch.nn import functional as F
import pandas as pd
import numpy as np
import torch
import csv


class PerturbationEncoder:

    def __init__(self, dataset_id, model_type, model_name):
        self.dataset_id = dataset_id
        self.model_type = model_type
        self.root = "../required_file/"

        if 'HUVEC' in self.dataset_id:
            self.sirna_to_gene_df = pd.read_csv(
                self.root+'ThermoFisher-export.csv')
            embedding_file = self.root+'perturbation_embedding_rxrx1.csv'
            with open(embedding_file) as csv_file:
                reader = csv.reader(csv_file)
                self.gene_to_embedding_dic = dict(reader)

        if 'BBBC021' in self.dataset_id:
            self.pert_to_embedding = pd.read_csv(self.root+
                'perturbation_embedding_bbb021.csv')
                
        if 'rohban' in self.dataset_id:
            self.pert_to_embedding = pd.read_csv(self.root+
                'perturbation_embedding_rohban.csv',
                header=None)
            self.pert_to_embedding.columns = ['gene_name'] + [str(i) for i in range(512)]
        
        if 'morphodiff' in self.dataset_id:
            self.pert_to_embedding = pd.read_csv('/home/pengrui/work_space_pengrui/project/RNA图像合成/reference_code/MorphoDiff/morphodiff/required_file/pr_embedding.csv')
            self.pert_to_embedding = self.pert_to_embedding.rename(columns={'Unnamed: 0':'cmap_name'})


    def get_embedding_for_rxrx1(self, identifier):
        """Return perturbation embedding based on input perturbation
        identifier.

        Args:
            identifier (str): perturbation identifier

        Raises:
            Exception: If there are no embeddings for the gene associated with
            input perturbation

        Returns:
            embedding (tensor): perturbation embedding
        """

        if identifier == 'EMPTY':
            # create an np array of size 512 and fill it with 1
            embedding = np.ones(512)

        else:
            if ';' in identifier:
                identifier = identifier.split(';')[0]
            gene = self.sirna_to_gene_df[
                self.sirna_to_gene_df['siRNA ID'] == identifier][
                    'Gene Symbol'].values[0]
            if ';' in gene:
                gene = gene.split(';')[0]

            if gene in self.gene_to_embedding_dic.keys():
                embedding = self.gene_to_embedding_dic[gene]
                embedding = embedding.replace('[', '').replace(']', '')
                embedding = np.fromstring(embedding, dtype=float, sep=' ')
            else:
                print("The "+gene+" gene for "+identifier+" is not in the dictionary.")
                raise Exception("The "+gene+" gene for "+identifier+" is not in the dictionary.")

        embedding = torch.from_numpy(embedding)

        if self.model_name == 'SD':
            embedding = self.standardize_gene_dimension(embedding)
        return embedding

    def get_embedding_for_bbbc021(self, identifier):
        """Return gene embedding based on input perturbation id.

        Args:
            identifier (str): perturbation identifier

        Returns:
            embedding (tensor): gene embedding
        """

        embedding = self.pert_to_embedding[
            self.pert_to_embedding['compound'] == identifier].values[0][1:]
        assert len(embedding) == self.pert_to_embedding.shape[1] - 1
        embedding = embedding.astype(float)
        embedding = torch.from_numpy(embedding)

        if self.model_name == 'SD':
            embedding = self.standardize_gene_dimension(embedding)
        return embedding
    
    def get_embedding_for_rohban(self, identifier):
        """Return gene embedding based on input perturbation id.

        Args:
            identifier (str): perturbation identifier

        Returns:
            embedding (tensor): gene embedding
        """

        embedding = self.pert_to_embedding[
            self.pert_to_embedding['gene_name'] == identifier].values[0][1:]
        assert len(embedding) == self.pert_to_embedding.shape[1] - 1
        embedding = embedding.astype(float)
        embedding = torch.from_numpy(embedding)

        if self.model_name == 'SD':
            embedding = self.standardize_gene_dimension(embedding)
        return embedding

    def standardize_gene_dimension(self, embedding):
        """Standardize gene embedding dimension to (bs, 77, 768).

        Args:
            gene_embedding (tensor): gene embedding generated by scGPT in
            512 dimensions

        Returns:
            padded_tensor (tensor): gene embedding padded with value 1 to have
            dimension (77, 768)"""
        # pad gene_embedding with value 1 to have dimension 768
        # (768 is SD prompt encoding size)
        final_size = 768

        # Calculate the amount of padding on each side
        padding_left = (final_size - len(embedding)) // 2
        padding_right = final_size - len(embedding) - padding_left
        # Pad the tensor
        padded_tensor = F.pad(
            embedding, (padding_left, padding_right), 'constant', 1)
        # replicate padded_tensor to have dimension (bs, 77, 768)
        padded_tensor = padded_tensor.repeat(1, 77, 1)
        return padded_tensor

    def get_gene_embedding(self, identifier):
        """Get gene embedding generated by scGPT based on input sirna id.

        Args:
            identifier (str): perturbation identifier

        Returns:
            embedding (tensor): perturbation embedding
        """
        embedding = []

        if self.model_type == 'conditional':

            if 'HUVEC' in self.dataset_id:
                embedding = self.get_embedding_for_rxrx1(identifier)

            elif 'BBBC021' in self.dataset_id:
                embedding = self.get_embedding_for_bbbc021(identifier)
                
            elif 'rohban' in self.dataset_id:
                embedding = self.get_embedding_for_rohban(identifier)

        elif self.model_type == 'naive':

            embedding = torch.ones(
                (1, 77, 768))

        else:

            raise Exception("Model type not recognized.")

        return embedding

class PerturbationEncoderInference:

    def __init__(self, dataset_id, model_type, model_name):
        self.dataset_id = dataset_id
        self.model_type = model_type
        self.model_name = model_name
        self.root = "../required_file/"

        if 'HUVEC' in self.dataset_id:
            self.sirna_to_gene_df = pd.read_csv(
                self.root+'ThermoFisher-export.csv')
            input_file = self.root+'perturbation_embedding_rxrx1.csv'
            with open(input_file) as csv_file:
                reader = csv.reader(csv_file)
                self.gene_to_embedding_dic = dict(reader)

        if 'BBBC021' in self.dataset_id:
            self.pert_to_embedding = pd.read_csv(self.root+
                'perturbation_embedding_bbbc021.csv')
                
        if 'rohban' in self.dataset_id:
            self.pert_to_embedding = pd.read_csv(self.root+
                'perturbation_embedding_rohban.csv', header=None)
            self.pert_to_embedding.columns = ['gene_name'] + [str(i) for i in range(512)]

        if 'morphodiff' in self.dataset_id:
            self.pert_to_embedding = pd.read_csv('/home/pengrui/work_space_pengrui/project/RNA图像合成/AIVCdiff/molecule_embeddings_rdkit_BBBC021.csv')
            self.pert_to_embedding = self.pert_to_embedding.rename(columns={'Unnamed: 0':'cmap_name'})

    def __call__(self, identifier):
        """Get gene embedding for input perturbation with unique identifier.

        Args:
            identifier (str): perturbation identifier

        Returns:
            embedding (tensor): perturbation embedding
        """
        embedding = []

        if self.model_name == 'SD':

            if self.model_type == 'conditional':

                if 'HUVEC' in self.dataset_id:
                    embedding = self.get_embedding_for_rxrx1(identifier)

                elif 'BBBC021' in self.dataset_id:
                    embedding = self.get_embedding_for_bbbc021(identifier)
                
                elif 'morphodiff' in self.dataset_id:
                    embedding = self.get_embedding_for_morphodiff(identifier)

                elif 'rohban' in self.dataset_id:
                    embedding = self.get_embedding_for_rohban(identifier)
                
            elif self.model_type == 'naive':

                batch_size = len(identifier)

                embedding = torch.ones(
                    (batch_size, 77, 768))
            
            # assert embedding.shape == (1, 77, 768)

        return embedding
    
    def get_embeddings_batch(self, identifiers):
        """批量返回多个扰动的基因嵌入向量

        Args:
            identifiers (list): 扰动标识符列表

        Returns:
            embeddings (tensor): 形状为 [batch_size, 77, 768] 的基因嵌入张量
        """
        embeddings_list = []
        
        for identifier in identifiers:
            embedding = self.pert_to_embedding[
                self.pert_to_embedding['compound'] == identifier].values[0][1:]
            embedding = torch.tensor(embedding.astype(float))
            embeddings_list.append(embedding)
        
        # 堆叠所有embedding [batch_size, embedding_dim]
        embeddings = torch.stack(embeddings_list)
        
        if self.model_name == 'SD':
            embeddings = self.standardize_gene_dimension_batch(embeddings)
            # 确保输出维度正确
            assert embeddings.shape == (len(identifiers), 77, 768), \
                f"Expected shape ({len(identifiers)}, 77, 768), got {embeddings.shape}"
        
        return embeddings

    def standardize_gene_dimension_batch(self, embeddings):
        """将批量基因嵌入维度标准化为(batch_size, 77, 768)

        Args:
            embeddings (tensor): 形状为 [batch_size, embedding_dim] 的基因嵌入张量

        Returns:
            padded_tensors (tensor): 形状为 [batch_size, 77, 768] 的填充后的基因嵌入张量
        """
        batch_size = embeddings.shape[0]
        final_size = 768

        # 计算需要在两侧填充的数量
        padding_left = (final_size - embeddings.shape[1]) // 2
        padding_right = final_size - embeddings.shape[1] - padding_left

        # 对批量数据进行填充
        padded_tensors = F.pad(
            embeddings, (padding_left, padding_right), 'constant', 1)  # [batch_size, 768]
        
        # 扩展维度并重复77次
        padded_tensors = padded_tensors.unsqueeze(1)  # [batch_size, 1, 768]
        padded_tensors = padded_tensors.repeat(1, 77, 1)  # [batch_size, 77, 768]

        return padded_tensors.float()

    def get_embedding_for_morphodiff(self, identifier):
        """返回单个扰动的基因嵌入向量

        Args:
            identifier (str): 扰动标识符

        Returns:
            embedding (tensor): 基因嵌入向量
        """
        return self.get_embeddings_batch(identifier)

    # def standardize_gene_dimension(self, embedding):
    #     """Standardize gene embedding dimension to (bs, 77, 768).

    #     Args:
    #         gene_embedding (tensor): gene embedding generated by scGPT in
    #         512 dimensions

    #     Returns:
    #         padded_tensor (tensor): gene embedding padded with value 1 to have
    #         dimension (77, 768)"""
    #     # pad gene_embedding with value 1 to have dimension 768
    #     # (768 is SD prompt encoding size)
    #     final_size = 768

    #     # Calculate the amount of padding on each side
    #     padding_left = (final_size - len(embedding)) // 2
    #     padding_right = final_size - len(embedding) - padding_left
    #     # Pad the tensor
    #     padded_tensor = F.pad(
    #         embedding, (padding_left, padding_right), 'constant', 1)
    #     # replicate padded_tensor to have dimension (bs, 77, 768)
    #     padded_tensor = padded_tensor.repeat(1, 77, 1).float()

    #     return padded_tensor

    def get_embedding_for_rxrx1(self, identifier):
        """Return gene embedding generated by scGPT based on input sirna id.

        Args:
            identifier (str): perturbation identifier

        Raises:
            Exception: If there are no embeddings for the gene associated
            with input sirna

        Returns:
            embedding (tensor): gene embedding generated by scGPT in 512
            dimensions
        """

        if identifier == 'EMPTY':
            embedding = np.ones(512)

        else:
            if ';' in identifier:
                identifier = identifier.split(';')[0]
            gene = identifier
            if ';' in gene:
                gene = gene.split(';')[0]

            if gene in self.gene_to_embedding_dic.keys():
                embedding = self.gene_to_embedding_dic[gene]
                embedding = embedding.replace('[', '').replace(']', '')
                embedding = np.fromstring(embedding, dtype=float, sep=' ')
            else:
                print("The "+gene+" gene for "+identifier+" is not in the dictionary.")
                raise Exception("The "+gene+" gene for "+identifier+" is not in the dictionary.")

        embedding = torch.from_numpy(embedding)

        if self.model_name == 'SD':
            embedding = self.standardize_gene_dimension(embedding)

        return embedding

    def get_embedding_for_bbbc021(self, identifier):
        """Return gene embedding based on input perturbation id.

        Args:
            identifier (str): perturbation identifier

        Returns:
            embedding (tensor): gene embedding
        """
        embedding = self.pert_to_embedding[
            self.pert_to_embedding['compound'] == identifier].values[0][1:]
        assert len(embedding) == self.pert_to_embedding.shape[1] - 1
        embedding = embedding.astype(float)
        embedding = torch.from_numpy(embedding)

        if self.model_name == 'SD':
            embedding = self.standardize_gene_dimension(embedding)

        return embedding
    
    def get_embedding_for_rohban(self, identifier):
        """Return gene embedding based on input perturbation id.

        Args:
            identifier (str): perturbation identifier

        Returns:
            embedding (tensor): gene embedding
        """
        assert identifier in self.pert_to_embedding['gene_name'].values
        embedding = self.pert_to_embedding[
            self.pert_to_embedding['gene_name'] == identifier].values[0][1:]
        assert len(embedding) == self.pert_to_embedding.shape[1] - 1
        embedding = embedding.astype(float)
        embedding = torch.from_numpy(embedding)

        if self.model_name == 'SD':
            embedding = self.standardize_gene_dimension(embedding)

        return embedding

